---
title: "Daily Log 22 May 2024"
subtitle: "Just a log of what I planned to do and what got done"
date: "2024-5-22"
---

## Plan

1. [00:30 AM - 02:00 AM] Read DM21 Paper. [1]
2. [02:00 AM - 09:30 AM] Sleep.
3. [10:00 AM - 12:00 PM] Samsung Hackathon Meeting.
4. [01:00 PM - 03:00 PM]
    - DFT PR-7 Fixing
    - ODE Solver Tutorial
5. [03:00 PM - 03:30 PM] Stethaim Meet.
5. [05:00 PM - 08:00 PM] Family Time.
6. [08:30 PM - 10:00 PM] DFS Meet.
7. [10:30 PM - 12:00 AM] DM21 First Blog.
8. [12:00 AM - 02:00 AM] DFT PR-8 Work

## New Things Learned

### ELU (Exponential Linear Unit)

<p align="center">
  <img src="https://latex.codecogs.com/svg.image?\
    f(x)= 
\begin{cases}
    x & \text{if } x > 0 \\
    \alpha(e^{x} - 1) & \text{if } x \leq 0
\end{cases}"/>
</p>

<p align="center">
  <img src="https://latex.codecogs.com/svg.image?\
    f(x)=
    \begin{cases}
    \end{cases}"/>
</p>

<p align="center">
  <img src="https://latex.codecogs.com/svg.image?\
  sin(x)"/>
</p>

I first time got to know about this non linear activation function in DM21 paper.

In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information.

Observations:
1. All values when x is a large number it gives of -1 as the result.

## Final
1. Mostly worked on reading DM21 Suplimentary docs. Wasted some time here and there.

2. I am facing some errors in representing formulas on the website, will look into it in the morning.

3. Samsung meeting went well, nothing much i have to answer the 4th question. And find some algorithm for forest fire. And this meet ended in just 30 min or less it started 30 min late as well.

4. Now i have some extra time so i will use this to fix the error in the site.

## References

[1] https://deepmind.google/discover/blog/simulating-matter-on-the-quantum-scale-with-ai/
